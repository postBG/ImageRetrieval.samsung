{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "773afacd",
   "metadata": {},
   "source": [
    "# Towards Content-based Image Retrieval Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d5261e",
   "metadata": {},
   "source": [
    "## Boilerplate for Colab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87079198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a25de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "% cd gdrive/MyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf40e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/postBG/ImageRetrieval.samsung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e6d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "% cd ImageRetrieval.samsung/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef62ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipython-unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e8f5e4",
   "metadata": {},
   "source": [
    "## Let's begin! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb38d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext ipython_unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a99e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import unittest\n",
    "import numpy as np\n",
    "import utils\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from utils import ImageHelper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9812419c",
   "metadata": {},
   "source": [
    "## Dataset: fashionMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b214ec61",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda52ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 32\n",
    "mean = (0.5,)\n",
    "std = (0.5,)\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.Resize((img_size, img_size)), \n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=mean, std=std)])\n",
    "image_helper = ImageHelper(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43976936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import FashionMNIST\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "trainset = FashionMNIST('./data', download=True, split='train', transform=transform)\n",
    "valset = FashionMNIST('./data', download=True, split='val', transform=transform)\n",
    "testset = FashionMNIST('./data', download=True, split='test', transform=transform)\n",
    "\n",
    "idx_to_class = trainset.idx_to_class\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde9cdd5",
   "metadata": {},
   "source": [
    "### Take a Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87082f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainset.classes)\n",
    "print(idx_to_class)\n",
    "print(f'train dataset size: {len(trainset)}')\n",
    "print(f'validation dataset size: {len(valset)}')\n",
    "print(f'test dataset size: {len(testset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c2f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label, idx = trainset[3]\n",
    "image_helper.show_img(img)\n",
    "print(f'label: {trainset.idx_to_class[label]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3edd7d",
   "metadata": {},
   "source": [
    "## Database & kNN Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21cc3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Database(object):\n",
    "    def __init__(self, dataloader, feature_extractor):\n",
    "        self.dataloader = dataloader\n",
    "        self.dataset = dataloader.dataset\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.images_indices, self.extracted_features, self.targets = self.extract_features_to_construct_database()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def extract_features_to_construct_database(self):\n",
    "        self.feature_extractor.eval()\n",
    "        all_indices = []\n",
    "        all_features = []\n",
    "        all_targets = []\n",
    "        for xs, ys, idxs in tqdm(self.dataloader):\n",
    "            features = self.feature_extractor(xs)\n",
    "            features = features.view(features.size(0), -1)\n",
    "            all_features.extend(features)\n",
    "            \n",
    "            all_indices.extend(idxs)\n",
    "            all_targets.extend(ys)\n",
    "        return torch.stack(all_indices), torch.stack(all_features), torch.stack(all_targets)\n",
    "    \n",
    "    def get_image_and_target(self, idx):\n",
    "        return self.dataset[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960b674b",
   "metadata": {},
   "source": [
    "### Practice 1: Implement k-nearest neighbor algorithm\n",
    "\n",
    "We will use the dot-product between two vectors as the distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNN_search(query: torch.Tensor, database: Database, k=10):\n",
    "    # query: batch_size * feature_size\n",
    "    # database.extracted_features: database_size * feature_size\n",
    "    distance_matrix = ?  # distance_matrix's shape should be batch_size * database_size\n",
    "    top_scores, most_similar_indices = distance_matrix.topk(k)\n",
    "    return top_scores, most_similar_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f24144",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%unittest_main\n",
    "from vector_search import kNN_search as solution_kNN_search\n",
    "\n",
    "def generate_random_tensor(rng, size):\n",
    "    return torch.tensor(rng.random(size=size))\n",
    "\n",
    "class DummyDatabase(object):\n",
    "    def __init__(self, database_size, feature_size):\n",
    "        rng = np.random.default_rng(10)\n",
    "        self.extracted_features = generate_random_tensor(rng, size=(database_size, feature_size))\n",
    "\n",
    "class TestkNN(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.batch_size = 5\n",
    "        self.feature_size = 32\n",
    "        self.database_size = 128\n",
    "        self.dummy_database = DummyDatabase(self.database_size, self.feature_size)\n",
    "    \n",
    "    def test_kNN_search_k10(self):\n",
    "        random_seed = 0\n",
    "        rng = np.random.default_rng(random_seed)\n",
    "        queries = generate_random_tensor(rng, size=(self.batch_size, self.feature_size))\n",
    "        top_scores, retrieved_indices = kNN_search(queries, self.dummy_database, k=10)\n",
    "        \n",
    "        sol_top_scores, sol_retrieved_indices = solution_kNN_search(queries, self.dummy_database, k=10)\n",
    "        print(retrieved_indices.size())\n",
    "        self.assertTrue(top_scores.equal(sol_top_scores))\n",
    "        self.assertTrue(retrieved_indices.equal(sol_retrieved_indices))\n",
    "        \n",
    "    def test_kNN_search_k5(self):\n",
    "        random_seed = 1\n",
    "        rng = np.random.default_rng(random_seed)\n",
    "        queries = generate_random_tensor(rng, size=(self.batch_size, self.feature_size))\n",
    "        top_scores, retrieved_indices = kNN_search(queries, self.dummy_database, k=5)\n",
    "        \n",
    "        sol_top_scores, sol_retrieved_indices = solution_kNN_search(queries, self.dummy_database, k=5)\n",
    "        print(retrieved_indices.size())\n",
    "        self.assertTrue(top_scores.equal(sol_top_scores))\n",
    "        self.assertTrue(retrieved_indices.equal(sol_retrieved_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d10f8a",
   "metadata": {},
   "source": [
    "### Practice 2: Implement a Simple Flatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbd8185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFlatter(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # x: batch_size * num_channel * img_size * img_size\n",
    "        batch_size = ?\n",
    "        flatten = ?  # flatten's shape should be batch_size * (num_channel * img_size * img_size)\n",
    "        return flatten\n",
    "    \n",
    "    def eval(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ef12da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%unittest_main\n",
    "from models import SimpleFlatter as SimpleFlatterSolution\n",
    "\n",
    "\n",
    "class TestSimpleFlatter(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.batch_size = 5\n",
    "        self.num_channel = 3  # RGB\n",
    "        self.img_size = 32\n",
    "    \n",
    "    def test_simple_flatter(self):\n",
    "        random_seed = 0\n",
    "        rng = np.random.default_rng(random_seed)\n",
    "        x = generate_random_tensor(rng, size=(self.batch_size, self.num_channel, self.img_size, self.img_size))\n",
    "        model = SimpleFlatter()\n",
    "        out = model(x)\n",
    "        \n",
    "        self.assertTupleEqual((self.batch_size, self.num_channel * self.img_size * self.img_size), out.size())\n",
    "        self.assertTrue(out.equal(x.view(self.batch_size, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4744176",
   "metadata": {},
   "source": [
    "## Your First Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e56a86f",
   "metadata": {},
   "source": [
    "### Create Your Database using Flatter and Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85ef29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatter = SimpleFlatter()\n",
    "database = Database(testloader, flatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, target, idx = trainset[2]\n",
    "print(\"This is you query image!\")\n",
    "print(f\"{idx_to_class[target]}: {target}\")\n",
    "image_helper.show_img(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7425122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = img.unsqueeze(0)  # [1, channel_size, img_size, img_size]\n",
    "query = flatter(x)\n",
    "top_scores, indices = kNN_search(query, database)\n",
    "print(f\"The retrieved indices: {indices} (size: {tuple(indices.size())})\")\n",
    "\n",
    "topk = 0\n",
    "retrieved_img, retrieved_label, retrieved_idx = database.get_image_and_target(indices[0][topk])\n",
    "print(f\"{idx_to_class[retrieved_label]}: {retrieved_label}\")\n",
    "image_helper.show_img(retrieved_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79cbb41",
   "metadata": {},
   "source": [
    "### Run All Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c75325",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_all_retrieval(feature_extractor, valloader, database, k=10):\n",
    "    all_query_labels = []\n",
    "    all_retrieved_indices = []\n",
    "    all_retrieved_labels = []\n",
    "\n",
    "    feature_extractor.eval()\n",
    "    for xs, ys, idxs in tqdm(valloader):\n",
    "        query_features = feature_extractor(xs)\n",
    "        most_similar_indices = kNN_search(query_features, database, k=k)[1]\n",
    "\n",
    "        all_query_labels.extend(ys)\n",
    "        all_retrieved_indices.extend(most_similar_indices)\n",
    "        all_retrieved_labels.extend(database.targets[most_similar_indices])\n",
    "\n",
    "    return torch.stack(all_query_labels), torch.stack(all_retrieved_indices), torch.stack(all_retrieved_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62737f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_labels, retrieved_indices, retreived_labels = run_all_retrieval(flatter, valloader, database, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54e5f0d",
   "metadata": {},
   "source": [
    "## How to Evaluate An Image Retrieval System: Mean Average Precision \n",
    "\n",
    "<img src='./resources/map.png' width='75%'  align='left'/><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc9aa1b",
   "metadata": {},
   "source": [
    "### Practice 3: Implement Your Own MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(label: int, retrieved_labels: np.array, eps=1e-12):\n",
    "    corrects = np.where(retrieved_labels == label, 1, 0)\n",
    "    cum_corrects = np.cumsum(corrects)\n",
    "    denominators = np.arange(1, len(corrects) + 1)\n",
    "    precisions = cum_corrects / denominators\n",
    "    average_prevision = ?\n",
    "    return average_prevision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ae7861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(labels, retrieved_labels):\n",
    "    labels, retrieved_labels = labels.cpu().numpy(), retrieved_labels.cpu().numpy()\n",
    "    aps = ?\n",
    "    aps = np.array(aps)\n",
    "    return np.mean(aps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a8dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%unittest_main\n",
    "from metrics import average_precision as solution_ap\n",
    "from metrics import mean_average_precision as solution_map\n",
    "\n",
    "\n",
    "def generate_random_integer_tensor(rng, size, low=0, high=10):\n",
    "    return torch.tensor(rng.integers(low=low, high=high, size=size))\n",
    "\n",
    "class TestmAP(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.batch_size = 5\n",
    "        self.num_classes = 10\n",
    "    \n",
    "    def test_AP(self):\n",
    "        random_seed = 0\n",
    "        k = 10\n",
    "        rng = np.random.default_rng(random_seed)\n",
    "        label = rng.integers(low=0, high=self.num_classes)\n",
    "        retrieved_lables = generate_random_integer_tensor(rng, size=k, high=self.num_classes)\n",
    "        \n",
    "        self.assertAlmostEqual(average_precision(label, retrieved_lables), solution_ap(label, retrieved_lables))\n",
    "        \n",
    "    \n",
    "    def test_mAP(self):\n",
    "        random_seed = 0\n",
    "        rng = np.random.default_rng(random_seed)\n",
    "        k = 10\n",
    "        \n",
    "        labels = generate_random_integer_tensor(rng, size=self.batch_size, high=self.num_classes)\n",
    "        retrieved_lables = generate_random_integer_tensor(rng, size=(self.batch_size, k), high=self.num_classes)\n",
    "        mean_ap = mean_average_precision(labels, retreived_labels)\n",
    "        \n",
    "        self.assertEqual(mean_ap, solution_map(labels, retreived_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f19d5bc",
   "metadata": {},
   "source": [
    "### Evaluate Simple Flatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25baf097",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_map = mean_average_precision(query_labels, retreived_labels)\n",
    "print(f\"Simple Flatter's mAP: {s_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851ad23",
   "metadata": {},
   "source": [
    "### Improve Simple Flatter: Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724e505",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedSimpleFlatter(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # x: batch_size * num_channel * img_size * img_size\n",
    "        batch_size = x.size(0)\n",
    "        flatten = x.view(batch_size, -1)  # flatten's shape should be batch_size * (num_channel * img_size * img_size)\n",
    "        return F.normalize(flatten)\n",
    "    \n",
    "    def eval(self):\n",
    "        pass\n",
    "    \n",
    "normed_flatter = NormalizedSimpleFlatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a0836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_database = Database(testloader, normed_flatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3cc1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_labels2, retrieved_indices2, retreived_labels2 = run_all_retrieval(normed_flatter, valloader, normed_database, k=10)\n",
    "print(f\"Normalized Flatter's mAP: {mean_average_precision(query_labels2, retreived_labels2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02f04e6",
   "metadata": {},
   "source": [
    "## Deep Learning Based Image Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974dd15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import FeatureExtractor, Classifier\n",
    "from torch.optim import Adam\n",
    "\n",
    "feature_extractor = FeatureExtractor()\n",
    "classifier = Classifier()\n",
    "model = nn.Sequential(feature_extractor, classifier)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5efa215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainers import train\n",
    "\n",
    "epoch = 10\n",
    "model = train(model, optimizer, trainloader, valloader, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc186e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_database = Database(testloader, feature_extractor)\n",
    "\n",
    "query_labels3, retrieved_indices3, retreived_labels3 = run_all_retrieval(feature_extractor, valloader, deep_database, k=10)\n",
    "deep_map = mean_average_precision(query_labels3, retreived_labels3)\n",
    "print(f\"Deep Feature Extractor's mAP: {deep_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec6f48b",
   "metadata": {},
   "source": [
    "## Compare the Image Retrieval Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eeca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Flatters database size: {database.extracted_features.numel()}')\n",
    "print(f'Deep database size: {deep_database.extracted_features.numel()}')\n",
    "print()\n",
    "print(f'The deep learning based system is {int(database.extracted_features.numel() / deep_database.extracted_features.numel())}x smaller,')\n",
    "print(\"while it's mAP is {:.2f}% better even on this simple dataset.\".format((deep_map - s_map) / s_map * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7aaabf",
   "metadata": {},
   "source": [
    "## Rotated Dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2711d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rotations import RotationPredDataset\n",
    "\n",
    "rot_trainset = RotationPredDataset(trainset)\n",
    "rot_valset = RotationPredDataset(valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b6d1af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img, rot_label, idx = rot_trainset[124]\n",
    "image_helper.show_img(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42870f1",
   "metadata": {},
   "source": [
    "### Practice (If time allows)\n",
    "\n",
    "Use the trained model above (`feature_extractor`) to run retrieval on `rotated_testset`.\n",
    "\n",
    "**Steps to implement**:\n",
    "1. Create `DataLoader`s with `rot_trainloader` and `rot_valloader`\n",
    "2. Create `FeatureExtractor` and `Classifier` (the number of classes is 4)\n",
    "3. Train the model.\n",
    "2. Create a `Database`.\n",
    "3. Run retrieval using the `feature_extractor`, `valloader`, and Database created in step 2 (use k=10).\n",
    "4. Calculate mAP score of step 3.\n",
    "5. Print the mAP score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338aecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_trainloader = ?\n",
    "rot_valloader = ?\n",
    "\n",
    "rot_feature_extractor = ?\n",
    "rot_classifier = ?\n",
    "rot_model = nn.Sequential(rot_feature_extractor, rot_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e3c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_optimizer = Adam(rot_model.parameters(), lr=0.001)\n",
    "rot_model = train(rot_model, rot_optimizer, rot_trainloader, rot_valloader, epoch=10)\n",
    "\n",
    "rot_database = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29617214",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_labels4, retrieved_indices4, retreived_labels4 = ?\n",
    "mean_average_precision(query_labels4, retreived_labels4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f9ffa7",
   "metadata": {},
   "source": [
    "### How are the results? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d270bb",
   "metadata": {},
   "source": [
    "## Self-supervision: Pretext Tasks\n",
    "\n",
    "### Rotation Prediction\n",
    "![rotation](resources/rotation.png)\n",
    "\n",
    "### Colorization\n",
    "![colorization](resources/colorization.png)\n",
    "\n",
    "### Jigsaw\n",
    "![jigsaw](resources/jigsaw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabf06fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
